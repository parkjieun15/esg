# app.py

import os
import json
import datetime as dt

import streamlit as st
import pandas as pd
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from openai import OpenAI

# ---------------------------------------
# 0. OpenAI í´ë¼ì´ì–¸íŠ¸ (í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY ì‚¬ìš©)
# ---------------------------------------
client = OpenAI()


# ---------------------------------------
# 1. ë°ì´í„° ë¡œë”©
# ---------------------------------------
@st.cache_data
def load_data(csv_path: str = "cache_merged_esg.csv"):
    df = pd.read_csv(csv_path)

    # ë‚ ì§œ ì»¬ëŸ¼ dt â†’ datetime ë³€í™˜
    if "dt" in df.columns:
        df["dt"] = pd.to_datetime(df["dt"], errors="coerce")
    else:
        st.error("dt ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CSVì— dt(ë‚ ì§œ) ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

    # ì„ë² ë”©ìš© í…ìŠ¤íŠ¸: clean_text â†’ summary â†’ title ìˆœìœ¼ë¡œ ë³´ì™„
    df["text_for_embed"] = (
        df.get("clean_text", "")
        .fillna(df.get("summary", ""))
        .fillna(df.get("title", ""))
        .fillna("")
    )

    # ê²°ì¸¡ê°’ ìµœì†Œí™”
    if "esg_tag" not in df.columns:
        df["esg_tag"] = "unknown"
    if "severity_ai" not in df.columns:
        df["severity_ai"] = 0.0
    if "tone" not in df.columns:
        df["tone"] = "unknown"

    return df


# ---------------------------------------
# 2. ì„ë² ë”© ëª¨ë¸ + FAISS ì¸ë±ìŠ¤ ë¹Œë“œ (ìºì‹œ)
# ---------------------------------------
@st.cache_resource
def build_embed_and_index(texts, cache_dir="faiss_cache"):
    os.makedirs(cache_dir, exist_ok=True)
    emb_path = os.path.join(cache_dir, "embeddings.npy")
    index_path = os.path.join(cache_dir, "faiss.index")

    # 1) ìºì‹œê°€ ìˆìœ¼ë©´: ë°”ë¡œ ë¡œë“œ
    if os.path.exists(emb_path) and os.path.exists(index_path):
        embs = np.load(emb_path).astype("float32")
        faiss.normalize_L2(embs)
        dim = embs.shape[1]
        index = faiss.IndexFlatIP(dim)
        index.add(embs)

        # ëª¨ë¸ì€ ë§¤ë²ˆ ë¡œë“œ (ë¹„êµì  ê°€ë²¼ì›€)
        model = SentenceTransformer(
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        )
        return model, embs, index

    # 2) ìºì‹œê°€ ì—†ìœ¼ë©´: ì²˜ìŒ í•œ ë²ˆë§Œ ê³„ì‚°
    model = SentenceTransformer(
        "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    )
    embs = model.encode(
        texts,
        convert_to_numpy=True,
        show_progress_bar=True,
    ).astype("float32")

    faiss.normalize_L2(embs)
    dim = embs.shape[1]
    index = faiss.IndexFlatIP(dim)
    index.add(embs)

    # ë””ìŠ¤í¬ì— ì €ì¥
    np.save(emb_path, embs)
    faiss.write_index(index, index_path)

    return model, embs, index


# ---------------------------------------
# 3. ì˜ë¯¸ê¸°ë°˜ ê²€ìƒ‰ í•¨ìˆ˜ (FAISS)
# ---------------------------------------
def semantic_search(query: str, model, index, df, top_k: int = 30):
    # ì§ˆì˜ê°€ ë¹„ì–´ ìˆìœ¼ë©´ ê·¸ëƒ¥ ì›ë³¸ df ë°˜í™˜
    if not query.strip():
        return df.copy()

    q_emb = model.encode([query], convert_to_numpy=True).astype("float32")
    faiss.normalize_L2(q_emb)

    D, I = index.search(q_emb, top_k)
    scores = D[0]
    idxs = I[0]

    rows = []
    for score, idx in zip(scores, idxs):
        row = df.iloc[int(idx)].copy()
        row["similarity"] = float(score)
        rows.append(row)

    if not rows:
        return pd.DataFrame(columns=list(df.columns) + ["similarity"])

    result = pd.DataFrame(rows)
    return result


# ---------------------------------------
# 4. ì§ˆì˜ íŒŒì‹± (ê¸°ì—…/ê¸°ê°„/í‚¤ì›Œë“œ/ì˜ë„ ì¶”ì¶œ)
# ---------------------------------------
def parse_user_query(query: str):
    """
    ì‚¬ìš©ì ìì—°ì–´ ì§ˆì˜ë¥¼ JSON êµ¬ì¡°ë¡œ íŒŒì‹±.
    company, period, esg_keywords, intent ë“±ì„ ì¶”ì¶œ.
    """
    if not query.strip():
        # ë¹„ì–´ ìˆìœ¼ë©´ ê¸°ë³¸ê°’
        return {
            "companies": [],
            "start_date": None,
            "end_date": None,
            "esg_keywords": [],
            "intent": "summary",
        }

    system_prompt = (
        "ë„ˆëŠ” í•œêµ­ì–´ ê¸ˆìœµÂ·ESG ì• ë„ë¦¬ìŠ¤íŠ¸ ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤. "
        "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ ê¸°ì—…ëª…, ê¸°ê°„, ESG ê´€ë ¨ í‚¤ì›Œë“œ, ì‚¬ìš© ì˜ë„ë¥¼ êµ¬ì¡°í™”í•˜ì—¬ JSONìœ¼ë¡œë§Œ ì¶œë ¥í•˜ë¼."
    )

    user_prompt = f"""
ì§ˆë¬¸: {query}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•´ë¼ (ì„¤ëª… ë¬¸ì¥ ì ˆëŒ€ ì“°ì§€ ë§ ê²ƒ).

{{
  "companies": ["ê¸°ì—…ëª…1", "ê¸°ì—…ëª…2"],   // ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´
  "start_date": "YYYY-MM-DD" ë˜ëŠ” null,   // 'ì‘ë…„', 'ìµœê·¼ 6ê°œì›”' ë“±ì€ ì ë‹¹íˆ í•´ì„
  "end_date": "YYYY-MM-DD" ë˜ëŠ” null,
  "esg_keywords": ["íƒ„ì†Œë°°ì¶œ", "í™˜ê²½ì˜¤ì—¼", ...],  // ê´€ë ¨ í‚¤ì›Œë“œ 3~7ê°œ
  "intent": "summary" ë˜ëŠ” "risk_focus" ë˜ëŠ” "comparison" ë˜ëŠ” "other"
}}
    """

    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.1,
        response_format={"type": "json_object"},
    )

    try:
        data = json.loads(resp.choices[0].message.content)
    except Exception:
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì•ˆì „í•œ ê¸°ë³¸ê°’
        data = {
            "companies": [],
            "start_date": None,
            "end_date": None,
            "esg_keywords": [],
            "intent": "summary",
        }
    return data


# ---------------------------------------
# 5. ê°œë³„ ê¸°ì‚¬ì— ëŒ€í•œ ESG ì •ëŸ‰ í‰ê°€
# ---------------------------------------
def llm_score_document(row: pd.Series, query_info: dict):
    """
    ë‹¨ì¼ ê¸°ì‚¬ì— ëŒ€í•´:
    - ESG ë¶„ë¥˜(E/S/G/mixed/none)
    - E/S/Gë³„ ì ìˆ˜ (0~3)
    - total_severity (0~9)
    - key_sentences (2~3ê°œ)
    ë¥¼ JSONìœ¼ë¡œ ë°›ì•„ì˜¨ë‹¤.
    """
    title = row.get("title", "")
    text = row.get("clean_text", row.get("summary", ""))
    date_str = str(row.get("dt", ""))[:10]

    qi = query_info

    scoring_rules = """
[ì‹¬ê°ë„ ê³„ì‚° ê·œì¹™]

ê° ì ìˆ˜ë¥¼ 0~3 ì‚¬ì´ ì •ìˆ˜ë¡œ ë¶€ì—¬í•œë‹¤. ê¸°ì¤€ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

0ì : ê´€ë ¨ ë¦¬ìŠ¤í¬ ê±°ì˜ ì—†ìŒ ë˜ëŠ” ë‹¨ìˆœ í™ë³´/ì¼ë°˜ ì†Œì‹
1ì : ì ì¬ì /ê²½ë¯¸í•œ ë¦¬ìŠ¤í¬ (íšŒì‚¬ ì‹¤ì ì´ë‚˜ í‰íŒì— í° ì˜í–¥ì€ ì ìŒ)
2ì : ì¤‘ê°„ ìˆ˜ì¤€ì˜ ë¦¬ìŠ¤í¬ (ê·œì œ, í‰íŒ, ë¹„ìš© ì¦ê°€ ë“± ê°€ì‹œì  ì˜í–¥ ê°€ëŠ¥)
3ì : ë§¤ìš° í° ë¦¬ìŠ¤í¬ (ë²•ì  ì œì¬, ëŒ€ê·œëª¨ ì‚¬ê³ , ESG ë“±ê¸‰ ê°•ë“± ê°€ëŠ¥ ìˆ˜ì¤€)

- E_score: í™˜ê²½(íƒ„ì†Œë°°ì¶œ, ì˜¤ì—¼, ì—ë„ˆì§€, ìì›, ê¸°í›„) ê´€ë ¨ ë¦¬ìŠ¤í¬ ì •ë„
- S_score: ì‚¬íšŒ(ì•ˆì „ì‚¬ê³ , ì¸ê¶Œ, ì§€ì—­ì‚¬íšŒ, ë…¸ë™, ê³ ê°/í˜‘ë ¥ì‚¬) ë¦¬ìŠ¤í¬ ì •ë„
- G_score: ì§€ë°°êµ¬ì¡°(ì´ì‚¬íšŒ, ì˜¤ë„ˆë¦¬ìŠ¤í¬, íšŒê³„ë¶€ì •, ë‚´ë¶€í†µì œ) ë¦¬ìŠ¤í¬ ì •ë„

total_severity = E_score + S_score + G_score (0~9)
"""

    prompt = f"""
ë„ˆëŠ” ê¸°ê´€íˆ¬ìì ëŒ€ìƒ í•œêµ­ ê¸°ì—… ESG ë¦¬ìŠ¤í¬ ë¶„ì„ê°€ë‹¤.

[ì‚¬ìš©ì ì§ˆì˜ ì •ë³´]
- íšŒì‚¬ í›„ë³´: {qi.get("companies", [])}
- ESG í‚¤ì›Œë“œ: {qi.get("esg_keywords", [])}
- ì˜ë„: {qi.get("intent", "")}

[ê¸°ì‚¬ ì •ë³´]
- ë‚ ì§œ: {date_str}
- ì œëª©: {title}
- ë³¸ë¬¸: {text}

ìœ„ ê¸°ì‚¬ ë‚´ìš©ë§Œì„ ê¸°ì¤€ìœ¼ë¡œ ESG ë¦¬ìŠ¤í¬ë¥¼ í‰ê°€í•˜ë¼.

{scoring_rules}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•´ë¼ (ì„¤ëª… ë¬¸ì¥ ì“°ì§€ ë§ ê²ƒ).

{{
  "esg_category": "E" ë˜ëŠ” "S" ë˜ëŠ” "G" ë˜ëŠ” "mixed" ë˜ëŠ” "none",
  "E_score": 0~3 ì •ìˆ˜,
  "S_score": 0~3 ì •ìˆ˜,
  "G_score": 0~3 ì •ìˆ˜,
  "total_severity": 0~9 ì •ìˆ˜,
  "key_sentences": ["ê¸°ì‚¬ì—ì„œ ì¸ìš©í•œ ì¤‘ìš”í•œ ë¬¸ì¥1", "ë¬¸ì¥2", "ë¬¸ì¥3"],
  "reason": "ì ìˆ˜ë¥¼ ì´ë ‡ê²Œ ì¤€ ì´ìœ ë¥¼ 2~3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½"
}}
    """

    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.1,
        response_format={"type": "json_object"},
    )

    try:
        data = json.loads(resp.choices[0].message.content)
    except Exception:
        data = {
            "esg_category": "none",
            "E_score": 0,
            "S_score": 0,
            "G_score": 0,
            "total_severity": 0,
            "key_sentences": [],
            "reason": "",
        }

    return data

def llm_summarize_top_docs(top_docs: pd.DataFrame, query_info: dict):
    """
    TOP-10 ë¬¸ì„œ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ESG ë¦¬í¬íŠ¸ë¡œ ìš”ì•½í•˜ëŠ” LLM í˜¸ì¶œ.
    """
    docs_context = []

    for _, row in top_docs.iterrows():
        # ë‚ ì§œ ì²˜ë¦¬
        if pd.notna(row.get("dt")):
            try:
                date_str = str(row["dt"].date())
            except Exception:
                date_str = str(row.get("dt"))
        else:
            date_str = ""

        # ìš”ì•½ìš© í…ìŠ¤íŠ¸ ì²˜ë¦¬ (í•­ìƒ ë¬¸ìì—´ë¡œ ë³€í™˜)
        text_src = row.get("clean_text")
        if not isinstance(text_src, str):
            text_src = row.get("summary")
        if not isinstance(text_src, str):
            text_src = ""
        text_src = str(text_src)
        text_snippet = text_src[:400]

        # severity_final (NaN â†’ 0)
        sev_val = row.get("severity_final", 0.0)
        try:
            sev_val = float(sev_val)
        except Exception:
            sev_val = 0.0

        docs_context.append(
            {
                "date": date_str,
                "title": row.get("title", ""),
                "esg_category": row.get("esg_llm", "none"),
                "severity_final": sev_val,
                "summary": text_snippet,
                "url": row.get("url", ""),
            }
        )

    # LLM ì…ë ¥ìš© JSON
    context_json = json.dumps(docs_context, ensure_ascii=False, indent=2)
    qi_str = json.dumps(query_info, ensure_ascii=False)

    prompt = f"""
ë„ˆëŠ” í•œêµ­ ëŒ€í˜• ê¸°ê´€íˆ¬ììì˜ ESG ì• ë„ë¦¬ìŠ¤íŠ¸ì´ë‹¤.

[ì‚¬ìš©ì ì§ˆì˜ ì •ë³´]
{qi_str}

[í›„ë³´ ë¬¸ì„œ 10ê°œ ìš”ì•½ ì •ë³´]
{context_json}

ê°œë³„ ê¸°ì‚¬ ì„¤ëª…ì€ í•˜ì§€ ë§ê³ ,
'ì „ì²´ì ìœ¼ë¡œ ì–´ë–¤ ESG ì´ìŠˆì™€ ë¦¬ìŠ¤í¬ê°€ ì¤‘ìš”í•œì§€'ë§Œ í•˜ë‚˜ì˜ ë¦¬í¬íŠ¸ë¡œ ì •ë¦¬í•˜ë¼.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥:
{{
  "overall_comment": "í•µì‹¬ ìš”ì•½ 3~5ë¬¸ì¥",
  "overall_key_sentences": ["ì¤‘ìš” ê·¼ê±° ë¬¸ì¥ 3~6ê°œ"],
  "risk_comment": "ì •ëŸ‰ ì‹¬ê°ë„(severity_final í‰ê· )ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íˆ¬ìì ì‹œì‚¬ì  2~3ë¬¸ì¥",
  "esg_focus": "E ë˜ëŠ” S ë˜ëŠ” G ë˜ëŠ” mixed",
  "representative_urls": ["ì¤‘ìš” ì°¸ê³ ìš© URL 3~5ê°œ"]
}}
"""

    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.1,
        response_format={"type": "json_object"},
    )

    try:
        data = json.loads(resp.choices[0].message.content)
    except Exception:
        data = {
            "overall_comment": "",
            "overall_key_sentences": [],
            "risk_comment": "",
            "esg_focus": "mixed",
            "representative_urls": [],
        }

    return data

# ---------------------------------------
# 6. Streamlit UI
# ---------------------------------------
def main():
    st.set_page_config(
        page_title="POSCO ESG ë‰´ìŠ¤ ì—ì´ì „íŠ¸",
        layout="wide",
    )

    st.title("ğŸ“Š POSCO ESG ë‰´ìŠ¤ ì—ì´ì „íŠ¸")

    # 6-1) ë°ì´í„° ë¡œë”© & ì„ë² ë”© ì¸ë±ìŠ¤ ì¤€ë¹„
    df = load_data()
    st.caption(f"ë°ì´í„° í–‰ ê°œìˆ˜: {len(df):,}ê±´")

    with st.spinner("ì„ë² ë”© ë° ê²€ìƒ‰ ì¸ë±ìŠ¤ë¥¼ ì¤€ë¹„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
        embed_model, embeddings, index = build_embed_and_index(
            df["text_for_embed"].tolist()
        )

    # ---------------------------
    # SIDEBAR - ìì—°ì–´ ì§ˆì˜ë§Œ ì‚¬ìš©
    # ---------------------------
    st.sidebar.header("ê²€ìƒ‰ / í•„í„° ì„¤ì •")

    # ğŸ” ìì—°ì–´ ì§ˆë¬¸ ì…ë ¥ë€
    user_query = st.sidebar.text_input(
        "ìì—°ì–´ë¡œ ì§ˆë¬¸í•´ë³´ì„¸ìš” (ì˜ˆ: 'ìµœê·¼ POSCO íƒ„ì†Œë°°ì¶œ ê´€ë ¨ ë¦¬ìŠ¤í¬ ì •ë¦¬í•´ì¤˜')",
        value="ìµœê·¼ í™˜ê²½ ë¦¬ìŠ¤í¬ ìš”ì•½í•´ì¤˜",
    )

    # ê²€ìƒ‰ ì‹¤í–‰ ë²„íŠ¼
    run_button = st.sidebar.button("ğŸ” ê²€ìƒ‰ ë° ë¶„ì„ ì‹¤í–‰")

    # -----------------------------------
    # MAIN ì˜ì—­ - ê²°ê³¼ ì¶œë ¥
    # -----------------------------------
    if not run_button:
        st.info("ì™¼ìª½ì—ì„œ ì§ˆë¬¸ì„ ì…ë ¥í•œ ë’¤ **[ğŸ” ê²€ìƒ‰ ë° ë¶„ì„ ì‹¤í–‰]** ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
        return

    # 1) ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ (FAISS, ê³ ì • Top-K í›„ë³´)
    TOP_K = 50  # í›„ë³´ ë¬¸ì„œ 50ê°œ ì¤‘ì—ì„œ ìµœì¢… TOP-10 ë½‘ê¸°
    with st.spinner("ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ ì¤‘..."):
        search_df = semantic_search(user_query, embed_model, index, df, top_k=TOP_K)

    if search_df.empty:
        st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë‹¤ë¥´ê²Œ ì¨ë³´ê±°ë‚˜ ë°ì´í„°ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")
        return

    # 2) ì‚¬ìš©ì ì§ˆì˜ íŒŒì‹± â†’ ê¸°ì—…ëª… / ê¸°ê°„ / í‚¤ì›Œë“œ / ì˜ë„ ë“± ì¶”ì¶œ
    with st.spinner("ì§ˆì˜ ë‚´ìš©ì„ ë¶„ì„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
        query_info = parse_user_query(user_query)

    filtered = search_df.copy()

    # 2-1) ìì—°ì–´ì—ì„œ íŒŒì•…í•œ ê¸°ê°„ìœ¼ë¡œ í•„í„°ë§ (start_date / end_date)
    if "dt" in filtered.columns:
        if query_info.get("start_date"):
            start_date = pd.to_datetime(query_info["start_date"]).date()
            filtered = filtered[filtered["dt"].dt.date >= start_date]
        if query_info.get("end_date"):
            end_date = pd.to_datetime(query_info["end_date"]).date()
            filtered = filtered[filtered["dt"].dt.date <= end_date]

    if filtered.empty:
        st.warning("ì§ˆì˜ì—ì„œ ì¶”ì¶œí•œ ê¸°ê°„ ì¡°ê±´ì„ ì ìš©í•˜ë‹ˆ ë‚¨ëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” ë„“ê²Œ ì ì–´ë³´ì„¸ìš”.")
        return

    # -----------------------------
    # 3) TOP-10 ë¬¸ì„œ ì„ ì • ë° LLM ì •ëŸ‰ ë¶„ì„
    # -----------------------------
    # similarity ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ 10ê°œë§Œ ì‚¬ìš©
    if "similarity" in filtered.columns:
        filtered = filtered.sort_values("similarity", ascending=False)

    top_docs = filtered.head(10).copy().reset_index(drop=True)

    # 3-2) ê° ë¬¸ì„œë³„ ESG ë¶„ë¥˜ + ì •ëŸ‰ ì‹¬ê°ë„ + ê·¼ê±°ë¬¸ì¥
    results = []
    with st.spinner("ì„ íƒëœ ê¸°ì‚¬ë“¤ì— ëŒ€í•´ ESG ë¶„ë¥˜ ë° ì •ëŸ‰ ì‹¬ê°ë„ í‰ê°€ ì¤‘ì…ë‹ˆë‹¤..."):
        for i, row in top_docs.iterrows():
            r = llm_score_document(row, query_info)
            results.append(r)

    # ê²°ê³¼ë¥¼ top_docsì— ë¶™ì´ê¸°
    top_docs["esg_llm"] = [r["esg_category"] for r in results]
    top_docs["E_score"] = [r["E_score"] for r in results]
    top_docs["S_score"] = [r["S_score"] for r in results]
    top_docs["G_score"] = [r["G_score"] for r in results]
    top_docs["severity_llm"] = [r["total_severity"] for r in results]
    top_docs["llm_reason"] = [r["reason"] for r in results]
    top_docs["llm_key_sentences"] = [r["key_sentences"] for r in results]

    # -----------------------------
    # 4) ê´€ë ¨ë„Â·ìµœì‹ ì„±Â·ìœ„í—˜ë„ì˜ **ê°€ì¤‘í•©**ìœ¼ë¡œ ìµœì¢… ì‹¬ê°ë„ ê³„ì‚°
    # -----------------------------

    # (1) ê´€ë ¨ë„: similarity â†’ 0~1 ì •ê·œí™”
    rel_raw = top_docs["similarity"].fillna(0.0)
    rel_min, rel_max = rel_raw.min(), rel_raw.max()
    if rel_max > rel_min:
        relevance = (rel_raw - rel_min) / (rel_max - rel_min)
    else:
        relevance = pd.Series(0.5, index=top_docs.index)  # ëª¨ë‘ ê°™ìœ¼ë©´ ì¤‘ê°„ê°’

    # (2) ìœ„í—˜ë„: LLM total_severity (0~9) â†’ 0~1
    risk = top_docs["severity_llm"].fillna(0.0) / 9.0
    risk = risk.clip(lower=0.0, upper=1.0)

    # (3) ìµœì‹ ì„±: ì˜¤ëŠ˜ ê¸°ì¤€ ë‚ ì§œ ì°¨ì´ â†’ 0~1
    today = pd.Timestamp.today().normalize()
    if "dt" in top_docs.columns and top_docs["dt"].notna().any():
        age_days = (today - top_docs["dt"].dt.normalize()).dt.days
        age_days = age_days.clip(lower=0)
        max_age = max(age_days.max(), 1)
        recency = 1.0 - (age_days / max_age)
        recency = recency.clip(lower=0.0, upper=1.0)
        # ë‚ ì§œê°€ NaTì¸ ê²½ìš° 0.5ë¡œ ëŒ€ì²´
        recency = recency.fillna(0.5)
    else:
        recency = pd.Series(0.5, index=top_docs.index)

    # (4) ê°€ì¤‘ì¹˜ ì„¤ì •
    # ê´€ë ¨ë„ 0.3, ìµœì‹ ì„± 0.2, ìœ„í—˜ë„ 0.5  â†’ í•©ì´ 1.0
    W_REL = 0.3
    W_REC = 0.2
    W_RISK = 0.5

    # (5) 0~1 ë²”ìœ„ì˜ ê°€ì¤‘í•© ì ìˆ˜
    score_01 = (
        W_REL * relevance +
        W_REC * recency +
        W_RISK * risk
    )

    # (6) ìµœì¢… ì‹¬ê°ë„: 0~10 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
    top_docs["relevance_factor"] = relevance
    top_docs["risk_factor"] = risk
    top_docs["recency_factor"] = recency
    top_docs["severity_final"] = 10.0 * score_01

    # -----------------------------
    # 5) ë‚ ì§œë³„ í‰ê·  ì‹¬ê°ë„ ê·¸ë˜í”„ (6ë²ˆ: ê·¸ë˜í”„)
    # -----------------------------
    st.subheader("ğŸ“ˆ ì„ íƒëœ ë¬¸ì„œë“¤ì˜ ë‚ ì§œë³„ í‰ê·  ì‹¬ê°ë„(severity_final) ì¶”ì´")

    trend_df = (
        top_docs.dropna(subset=["dt"])
        .assign(date=lambda x: x["dt"].dt.date)
        .groupby("date")["severity_final"]
        .mean()
        .sort_index()
    )

    if not trend_df.empty:
        st.line_chart(trend_df)
    else:
        st.write("ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ìˆ˜ ìˆëŠ” ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # -----------------------------
    # -----------------------------
    # 6) TOP-10 í†µí•© ë¦¬í¬íŠ¸ (í•˜ë‚˜ë§Œ ë³´ì—¬ì¤Œ)
    # -----------------------------
    # ìˆ«ì ìš”ì•½ê°’ë“¤ ë¨¼ì € ê³„ì‚°
    avg_E = top_docs["E_score"].mean()
    avg_S = top_docs["S_score"].mean()
    avg_G = top_docs["G_score"].mean()
    overall_sev = top_docs["severity_final"].mean()

    with st.spinner("TOP-10 ë¬¸ì„œë¥¼ í†µí•© ë¶„ì„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
        summary = llm_summarize_top_docs(top_docs, query_info)

    esg_focus = summary.get("esg_focus", "mixed")
    overall_comment = summary.get("overall_comment", "")
    key_sents = summary.get("overall_key_sentences", []) or []
    risk_comment = summary.get("risk_comment", "")
    rep_urls = summary.get("representative_urls", []) or []

    # ëŒ€í‘œ URLì´ ê±°ì˜ ì—†ìœ¼ë©´, top_docsì—ì„œ ìƒìœ„ 5ê°œ URLì„ ì±„ì›Œë„£ê¸°
    if not rep_urls:
        rep_urls = [
            u for u in top_docs["url"].dropna().unique().tolist()
            if isinstance(u, str) and u
        ][:5]

    st.subheader("ğŸ“˜ TOP-10 í†µí•© ESG ë¦¬í¬íŠ¸")

    col1, col2 = st.columns([3, 1])

    with col1:
        # 1) ESG ë¶„ë¥˜ ê²°ê³¼
        st.markdown("**1) ESG ë¶„ë¥˜ ê²°ê³¼**")
        st.write(f"- LLMì´ íŒë‹¨í•œ ì£¼ìš” ì´ˆì  ì˜ì—­: {esg_focus}")
        st.write(
            f"- Top10 í‰ê·  E/S/G ì ìˆ˜: "
            f"E={avg_E:.2f}, S={avg_S:.2f}, G={avg_G:.2f}"
        )

        # 2) ì •ëŸ‰ ì‹¬ê°ë„ (ê°€ì¤‘í•©)
        st.markdown("**2) ì •ëŸ‰ ì‹¬ê°ë„ (ê°€ì¤‘í•©)**")
        st.write(
            "- ê´€ë ¨ë„Â·ìµœì‹ ì„±Â·ìœ„í—˜ë„ë¥¼ ê°€ì¤‘í•©(0.3/0.2/0.5)í•œ ê°’ì˜ "
            f"í‰ê· ì…ë‹ˆë‹¤.\n- Top10 í‰ê·  í†µí•© ì‹¬ê°ë„(severity_final): "
            f"**{overall_sev:.2f} / 10**"
        )
        if risk_comment:
            st.write(risk_comment)

        # 3) ìš”ì•½ / ì½”ë©˜íŠ¸
        st.markdown("**3) ìš”ì•½ / ì½”ë©˜íŠ¸**")
        st.write(overall_comment)

        # 4) ê·¼ê±° ë¬¸ì¥
        st.markdown("**4) ê·¼ê±° ë¬¸ì¥ (key sentences)**")
        if key_sents:
            for s in key_sents:
                st.write(f"- {s}")
        else:
            st.write("- (LLMì´ ê·¼ê±° ë¬¸ì¥ì„ ë³„ë„ë¡œ ì œì‹œí•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.)")

    with col2:
        # 5) ì¶œì²˜ ë§í¬
        st.markdown("**5) ëŒ€í‘œ ì¶œì²˜ ë§í¬**")
        if rep_urls:
            for u in rep_urls:
                st.markdown(f"- [ë§í¬]({u})")
        else:
            st.write("- (ëŒ€í‘œ ë§í¬ê°€ ì—†ìŠµë‹ˆë‹¤.)")

if __name__ == "__main__":
    main()
